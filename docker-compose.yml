services:
  app:
    build: .
    env_file: .env
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
      kafka:
        condition: service_started
    command: ["python", "-m", "ugc_bot.app"]
    restart: unless-stopped

  migrate:
    build: .
    env_file: .env
    depends_on:
      db:
        condition: service_healthy
    command: ["alembic", "upgrade", "head"]
    restart: "no"

  admin:
    build: .
    env_file: .env
    depends_on:
      db:
        condition: service_healthy
    command:
      [
        "uvicorn",
        "ugc_bot.admin.app:app",
        "--host",
        "0.0.0.0",
        "--port",
        "8001",
        "--proxy-headers",
        "--forwarded-allow-ips",
        "*",
      ]
    ports:
      - "8001:8001"
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import urllib.request; urllib.request.urlopen('http://localhost:8001/admin/login', timeout=5)\" || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    networks:
      - default

  instagram_webhook:
    build: .
    env_file: .env
    depends_on:
      db:
        condition: service_healthy
    command: ["uvicorn", "ugc_bot.instagram_webhook_app:app", "--host", "0.0.0.0", "--port", "8002"]
    expose:
      - "8002"
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import urllib.request; urllib.request.urlopen('http://localhost:8002/webhook/instagram?hub.mode=subscribe&hub.challenge=test', timeout=5)\" || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    networks:
      - default

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - certbot_data:/etc/letsencrypt
      - certbot_www:/var/www/certbot
    depends_on:
      - instagram_webhook
    restart: unless-stopped
    networks:
      - default

  certbot:
    image: certbot/certbot
    volumes:
      - certbot_data:/etc/letsencrypt
      - certbot_www:/var/www/certbot
    entrypoint: "/bin/sh -c 'trap exit TERM; while :; do certbot renew; sleep 12h & wait $${!}; done;'"
    restart: unless-stopped
    networks:
      - default

  kafka_consumer:
    build: .
    env_file: .env
    depends_on:
      db:
        condition: service_healthy
      kafka:
        condition: service_started
    command: ["python", "-m", "ugc_bot.kafka_consumer"]
    restart: unless-stopped

  feedback_scheduler:
    build: .
    env_file: .env
    depends_on:
      db:
        condition: service_healthy
    command: ["python", "-m", "ugc_bot.feedback_scheduler"]
    restart: unless-stopped

  outbox_processor:
    build: .
    env_file: .env
    depends_on:
      db:
        condition: service_healthy
      kafka:
        condition: service_started
    command: ["python", "-m", "ugc_bot.outbox_processor"]
    restart: unless-stopped

  db:
    image: postgres:16-alpine
    environment:
      POSTGRES_DB: ugc
      POSTGRES_USER: ugc
      POSTGRES_PASSWORD: ugc
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ugc -d ugc"]
      interval: 5s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    restart: unless-stopped

  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.1
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    restart: unless-stopped

  kafka:
    image: confluentinc/cp-kafka:7.6.1
    depends_on:
      zookeeper:
        condition: service_started
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:19093
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
    ports:
      - "9092:9092"
      - "19093:9093"
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "kafka-topics --bootstrap-server localhost:9092 --list || exit 1",
        ]
      interval: 10s
      timeout: 5s
      retries: 10
    restart: unless-stopped


  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./config/monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./config/monitoring/alerts.yml:/etc/prometheus/alerts.yml:ro
      - prometheus_data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
    ports:
      - "9090:9090"
    depends_on:
      - node_exporter
      - cadvisor
      - alertmanager
    restart: unless-stopped
    networks:
      - default

  alertmanager_renderer:
    image: alpine:3.20
    environment:
      TELEGRAM_BOT_TOKEN: ${TELEGRAM_BOT_TOKEN}
      TELEGRAM_CHAT_ID: ${TELEGRAM_CHAT_ID}
      TELEGRAM_CRITICAL_CHAT_ID: ${TELEGRAM_CRITICAL_CHAT_ID}
    volumes:
      - ./config/monitoring/alertmanager.yml:/templates/alertmanager.yml:ro
      - alertmanager_config:/etc/alertmanager
    entrypoint: ["/bin/sh", "-c"]
    command: "apk add --no-cache gettext >/dev/null && envsubst < /templates/alertmanager.yml > /etc/alertmanager/alertmanager.yml"
    restart: "no"
    networks:
      - default

  alertmanager:
    image: prom/alertmanager:latest
    environment:
      TELEGRAM_BOT_TOKEN: ${TELEGRAM_BOT_TOKEN}
      TELEGRAM_CHAT_ID: ${TELEGRAM_CHAT_ID}
      TELEGRAM_CRITICAL_CHAT_ID: ${TELEGRAM_CRITICAL_CHAT_ID}
    volumes:
      - alertmanager_config:/etc/alertmanager
    command:
      - "--config.file=/etc/alertmanager/alertmanager.yml"
    depends_on:
      alertmanager_renderer:
        condition: service_completed_successfully
    ports:
      - "9093:9093"
    restart: unless-stopped
    networks:
      - default

  grafana:
    image: grafana/grafana:latest
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-admin}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/monitoring/grafana-datasource.yml:/etc/grafana/provisioning/datasources/datasource.yml:ro
      - ./config/monitoring/grafana-dashboard.yml:/etc/grafana/provisioning/dashboards/dashboards.yml:ro
      - ./config/monitoring/dashboards:/var/lib/grafana/dashboards:ro
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
    restart: unless-stopped
    networks:
      - default

  node_exporter:
    image: prom/node-exporter:latest
    command:
      - "--path.rootfs=/host"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/host:ro
    restart: unless-stopped
    networks:
      - default

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    restart: unless-stopped
    networks:
      - default

volumes:
  postgres_data:
  redis_data:
  certbot_data:
  certbot_www:
  prometheus_data:
  grafana_data:
  alertmanager_config:

networks:
  default:
    driver: bridge